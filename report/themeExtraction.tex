\section{Theme Extraction}
\label{sec:themeExtraction}

\paragraph{}
Once the data is extracted by the parser, the first task we need to tackle is to detect themes in the data collection. This is also known as topics clustering and is done by performing soft clustering on the words probability distributions.

\paragraph{}
Because of the very large time span of the collection, detecting a precise theme over the entire collection is almost impossible. To overcome this issue, the method in \cite{kdd05-ttm} splits the collection into several sub-collections $C_{i}$ (that we name \emph{time partition}) where the themes are extracted independently from each other.

\subsection{Probabilistic Model}

\paragraph{}
Themes are extracted from every time partition $C_{i}$ using a probabilistic mixture model and the expectation-maximization algorithm where the themes are the parameters to estimate. The latent variables of this model are k themes  $\theta_{1}$,...,$\theta_{k}$ and a background model $\theta_{B}$ for the whole collection. The presence of the background model variable is justified by the need to remove non-discriminating and non-informative words to describe a theme. The proportion of the background model is regulated by the mixing weight $\lambda_{B}$.

\paragraph{}
The goal of the algorithm is to maximize the following log-likelihood in a model:

\begin{center}

$
\sum_{d\in C_{i}} \sum_{w} [c(w,d)log(\lambda_{B}p(w|\theta_{B}) + (1-\lambda_{B})\sum_{j=1}^{k} (\pi_{d,j} p(w|\theta_{j})))]  
$

\end{center}

where 

\begin{itemize}
\item c(w,d) is the count of word w in document d
\item $p(w|\theta_{j})$ is the probability of having the word w in the theme j, knowing the theme distribution $\theta_{j}$
\item $p(w|\theta_{B})$ is the probability of having w in the background model, knowing $\theta_{B}$
\item $\pi_{d,j}$ is the probability that document d belongs to theme j
\end{itemize}

\paragraph{}
By initializing the $p(w|\theta_{j})$ randomly and the $\pi_{d,j}$ uniformly, we can perform several iterations of the expectation-maximization algorithm in order to maximize the log-likelihood. One iteration is given by the following formulas to update the latent probabilities:

\begin{equation*}
p(z_{d,w} = j) = \frac{\pi_{d,j}p(w|\theta_{j})} {\sum_{j\prime=1}^{k} {\pi_{d,j\prime}p(w|\theta_{j\prime})} }
\end{equation*}

\begin{equation*}
p(z_{d,w} = B) = \frac
{\lambda_{B} p(w|\theta_{B})} 
{\lambda_{B} p(w|\theta_{B}) + (1-\lambda_{B})
\sum_{j\prime=1}^{k} {\pi_{d,j\prime}p(w|\theta_{j\prime})}}
\end{equation*}

\begin{equation*}
\pi_{d,j} = \frac
{\sum_{w}{c(w,d)(1-p(z_{d,w} = B))(p(z_{d,w} = j))}}
{\sum_{j\prime=1}^{k}{\sum_{w}{c(w,d)(1-p(z_{d,w} = B))(p(z_{d,w} = j\prime))}}}
\end{equation*}

\begin{equation*}
p(w|\theta_{j}) = \frac
{\sum_{d\in C_{i}}{c(w,d)(1-p(z_{d,w} = B))(p(z_{d,w} = j))}}
{\sum_{w\prime}{
\sum_{d \in C_{i}} {c(w\prime,d)(1-p(z_{d,w\prime} = B))(p(z_{d,w\prime} = j))}}}
\end{equation*}


\paragraph{}
Because the convergence of the EM algorithm is not guaranteed, we need to run several times the algorithm with different initial probabilities.

\paragraph{}
The same model is applied to every sub-collection independently from each other. Only the background model is common but it is not updated during the iterations of the EM algorithm. 

\paragraph{}
Finally, we compute for each theme the average probability that documents belong to this theme. This enables us to filter some of them out for future steps.

\subsection{Implementation with Spark}

\paragraph{}
The fact of having multiple time partitions dividing the whole collection of articles is well suited for parallelization. Indeed, for each time partition (class EmInput), we process one expectation-maximization algorithm to its articles. There are no dependencies between different time partitions during the execution of the algorithm.

\paragraph{}
The class EmAlgo clones every EmInputs depending on the number of trials we choose and launch the EM algorithm concurrently. All iterations of one EmInput are performed within one map function to avoid time losses between executors or re-computations if Spark drops an output. Then, EmAlgo chooses the EmInput object that has the highest log-likelihood for each time period.

\paragraph{}
By testing our implementation with different parameters, we could see that it gives nice results with a mixing weight of the background model around 0.92. A higher value removes very important words (such as "war" in war time) and a lower value adds a lot of verbs to the results ; contrary to English, in French the informative words are mostly nouns not verbs.

\subsection{Complexity}

\paragraph{}
Once all the articles are parsed and the time partitions generated, the complexity and computation time of the expectation-maximization algorithm depend mainly on the number of articles, the number of themes and the number of iterations of the EM algorithm.
The size of the background model has also a non-negligible influence as the algorithm is going through the map multiple times at each iteration. However, this size doesn't grow linearly with the number of articles and we can clean words from every set of probabilities if the appearance doesn't exceed a minimum threshold.

\paragraph{}
In order to maintain efficient scalability, we choose a period defining the time partition of one week. The choice of this time period is a good trade-off as it is precise enough to locate events and enables us to have enough articles in each time period. Indeed, it gives us between 100 and 200 articles in each time partition to process which take about one minute to extract 10 themes.

\subsection{Results}

\paragraph{}
After running the expectation-maximization algorithm on specified time periods, we could extract relevant themes. We could observe that if there was no clear event at this time, it was not always easy to guess the meaning of the dectected theme, sometimes because it referred to precise events unknown from us, and sometimes because it was only consisting of noise. Fortunately, most of these themes can be filtered out based on their score. Some other themes provided real clear descriptions and we could easily know to which event the theme was referring.

Here are some example of relevant themes at the beginning of World War I in 1914:

\begin{center}
\begin{tabular}{|l|l|l|}
  \hline
  Jul 26 - Aug 3 & Aug 3 - Aug 10 & Aug 10 - Aug 17 \\
  \hline
  juillet & allemagne & allemands \\
  berlin & france & troupes \\
  mobilisation & frontière & guerre \\
  situation & mobilisation & français \\
  vienne & guerre & allemandes \\
  belgrade & français & ambassadeur \\
  guerre & luxembourg & pétersbourg \\
  russie & neutralité & bruxelles \\
  \hline
\end{tabular}
\end{center}

\paragraph{}
In addition, when the vocabular changes a lot in some set of articles, themes are well descripted. Here is a theme describing the first landing on the moon which was huge event and a theme describing the Panama Treaty in 1978, which was less covered by the press.


\begin{center}
\begin{tabular}{|l|l|}
  \hline
  Jul 17 - Jul 24 1969 & Jan 29 - Feb 5 1978 \\
  \hline
  apollo & panama \\
  lunaire & traités \\
  armstrong & ratification \\
  houston & frolinat \\
  aldrin & rebelles \\
  espace & carter \\
  \hline
\end{tabular}
\end{center}


